{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m491.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (23.5.26)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (3.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.6.3)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.26.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.60.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (4.9.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow==2.11.0) (59.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.25.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.5.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.31.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.25.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/leducmanh/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/leducmanh/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/leducmanh/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/leducmanh/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/leducmanh/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/leducmanh/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.0)\n",
      "Installing collected packages: tensorflow-estimator, protobuf, keras, gast, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.15.0\n",
      "    Uninstalling tensorflow-estimator-2.15.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.8.0\n",
      "    Uninstalling keras-2.8.0:\n",
      "      Successfully uninstalled keras-2.8.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "Successfully installed gast-0.4.0 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-io==0.25.0\n",
      "  Using cached tensorflow_io-0.25.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.25.0 in /home/leducmanh/.local/lib/python3.10/site-packages (from tensorflow-io==0.25.0) (0.25.0)\n",
      "Installing collected packages: tensorflow-io\n",
      "Successfully installed tensorflow-io-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.11.0\n",
    "!pip install tensorflow-io==0.30.0\n",
    "!pip install protobuf==3.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-io version: 0.25.0\n",
      "tensorflow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
    "print(\"tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -sSOL https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "          #  labels\n",
    "           'class',\n",
    "          #  low-level features\n",
    "           'lepton_1_pT',\n",
    "           'lepton_1_eta',\n",
    "           'lepton_1_phi',\n",
    "           'lepton_2_pT',\n",
    "           'lepton_2_eta',\n",
    "           'lepton_2_phi',\n",
    "           'missing_energy_magnitude',\n",
    "           'missing_energy_phi',\n",
    "          #  high-level derived features\n",
    "           'MET_rel',\n",
    "           'axial_MET',\n",
    "           'M_R',\n",
    "           'M_TR_2',\n",
    "           'R',\n",
    "           'MT2',\n",
    "           'S_R',\n",
    "           'M_Delta_R',\n",
    "           'dPhi_r_b',\n",
    "           'cos(theta_r1)'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lepton_1_pT</th>\n",
       "      <th>lepton_1_eta</th>\n",
       "      <th>lepton_1_phi</th>\n",
       "      <th>lepton_2_pT</th>\n",
       "      <th>lepton_2_eta</th>\n",
       "      <th>lepton_2_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>MET_rel</th>\n",
       "      <th>axial_MET</th>\n",
       "      <th>M_R</th>\n",
       "      <th>M_TR_2</th>\n",
       "      <th>R</th>\n",
       "      <th>MT2</th>\n",
       "      <th>S_R</th>\n",
       "      <th>M_Delta_R</th>\n",
       "      <th>dPhi_r_b</th>\n",
       "      <th>cos(theta_r1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972861</td>\n",
       "      <td>0.653855</td>\n",
       "      <td>1.176225</td>\n",
       "      <td>1.157156</td>\n",
       "      <td>-1.739873</td>\n",
       "      <td>-0.874309</td>\n",
       "      <td>0.567765</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>-0.252552</td>\n",
       "      <td>1.921887</td>\n",
       "      <td>0.889637</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>1.145621</td>\n",
       "      <td>1.932632</td>\n",
       "      <td>0.994464</td>\n",
       "      <td>1.367815</td>\n",
       "      <td>0.040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.667973</td>\n",
       "      <td>0.064191</td>\n",
       "      <td>-1.225171</td>\n",
       "      <td>0.506102</td>\n",
       "      <td>-0.338939</td>\n",
       "      <td>1.672543</td>\n",
       "      <td>3.475464</td>\n",
       "      <td>-1.219136</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>3.775174</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>0.568051</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.205356</td>\n",
       "      <td>1.321893</td>\n",
       "      <td>0.377584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444840</td>\n",
       "      <td>-0.134298</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-1.613871</td>\n",
       "      <td>-0.768661</td>\n",
       "      <td>1.219918</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>1.831248</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.526283</td>\n",
       "      <td>0.941514</td>\n",
       "      <td>1.587535</td>\n",
       "      <td>2.024308</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>1.562374</td>\n",
       "      <td>1.135454</td>\n",
       "      <td>0.180910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381256</td>\n",
       "      <td>-0.976145</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.448959</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>-0.677328</td>\n",
       "      <td>2.033060</td>\n",
       "      <td>1.533041</td>\n",
       "      <td>3.046260</td>\n",
       "      <td>-1.005285</td>\n",
       "      <td>0.569386</td>\n",
       "      <td>1.015211</td>\n",
       "      <td>1.582217</td>\n",
       "      <td>1.551914</td>\n",
       "      <td>0.761215</td>\n",
       "      <td>1.715464</td>\n",
       "      <td>1.492257</td>\n",
       "      <td>0.090719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.309996</td>\n",
       "      <td>-0.690089</td>\n",
       "      <td>-0.676259</td>\n",
       "      <td>1.589283</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>0.622907</td>\n",
       "      <td>1.087562</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>0.589204</td>\n",
       "      <td>1.365479</td>\n",
       "      <td>1.179295</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083158</td>\n",
       "      <td>0.043429</td>\n",
       "      <td>1.154854</td>\n",
       "      <td>0.094859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  lepton_1_pT  lepton_1_eta  lepton_1_phi  lepton_2_pT  lepton_2_eta  \\\n",
       "0    0.0     0.972861      0.653855      1.176225     1.157156     -1.739873   \n",
       "1    1.0     1.667973      0.064191     -1.225171     0.506102     -0.338939   \n",
       "2    1.0     0.444840     -0.134298     -0.709972     0.451719     -1.613871   \n",
       "3    1.0     0.381256     -0.976145      0.693152     0.448959      0.891753   \n",
       "4    1.0     1.309996     -0.690089     -0.676259     1.589283     -0.693326   \n",
       "\n",
       "   lepton_2_phi  missing_energy_magnitude  missing_energy_phi   MET_rel  \\\n",
       "0     -0.874309                  0.567765           -0.175000  0.810061   \n",
       "1      1.672543                  3.475464           -1.219136  0.012955   \n",
       "2     -0.768661                  1.219918            0.504026  1.831248   \n",
       "3     -0.677328                  2.033060            1.533041  3.046260   \n",
       "4      0.622907                  1.087562           -0.381742  0.589204   \n",
       "\n",
       "   axial_MET       M_R    M_TR_2         R       MT2       S_R  M_Delta_R  \\\n",
       "0  -0.252552  1.921887  0.889637  0.410772  1.145621  1.932632   0.994464   \n",
       "1   3.775174  1.045977  0.568051  0.481928  0.000000  0.448410   0.205356   \n",
       "2  -0.431385  0.526283  0.941514  1.587535  2.024308  0.603498   1.562374   \n",
       "3  -1.005285  0.569386  1.015211  1.582217  1.551914  0.761215   1.715464   \n",
       "4   1.365479  1.179295  0.968218  0.728563  0.000000  1.083158   0.043429   \n",
       "\n",
       "   dPhi_r_b  cos(theta_r1)  \n",
       "0  1.367815       0.040714  \n",
       "1  1.321893       0.377584  \n",
       "2  1.135454       0.180910  \n",
       "3  1.492257       0.090719  \n",
       "4  1.154854       0.094859  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "susy_iterator = pd.read_csv('./SUSY.csv.gz', header=None, names=COLUMNS, chunksize=100000)\n",
    "susy_df = next(susy_iterator)\n",
    "susy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 19)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of datapoints and columns\n",
    "len(susy_df), len(susy_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54025, 45975)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of datapoints belonging to each class (0: background noise, 1: signal)\n",
    "len(susy_df[susy_df[\"class\"]==0]), len(susy_df[susy_df[\"class\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  60000\n",
      "Number of testing sample:  40000\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(susy_df, test_size=0.4, shuffle=True)\n",
    "print(\"Number of training samples: \",len(train_df))\n",
    "print(\"Number of testing sample: \",len(test_df))\n",
    "\n",
    "x_train_df = train_df.drop([\"class\"], axis=1)\n",
    "y_train_df = train_df[\"class\"]\n",
    "\n",
    "x_test_df = test_df.drop([\"class\"], axis=1)\n",
    "y_test_df = test_df[\"class\"]\n",
    "\n",
    "# The labels are set as the kafka message keys so as to store data\n",
    "# in multiple-partitions. Thus, enabling efficient data retrieval\n",
    "# using the consumer groups.\n",
    "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "\n",
    "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 40000, 40000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_COLUMNS = len(x_train_df.columns)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 60000 messages into topic: turtorial11-susy-train\n",
      "Wrote 40000 messages into topic: turtorial11-susy-test\n"
     ]
    }
   ],
   "source": [
    "def error_callback(exc):\n",
    "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
    "\n",
    "def write_to_kafka(topic_name, items):\n",
    "  count=0\n",
    "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
    "  for message, key in items:\n",
    "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
    "    count+=1\n",
    "  producer.flush()\n",
    "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
    "\n",
    "write_to_kafka(\"turtorial11-susy-train\", zip(x_train, y_train))\n",
    "write_to_kafka(\"turtorial11-susy-test\", zip(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:33:16.013825: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2023-12-18 01:33:16.014141: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2023-12-18 01:33:16.148227: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-12-18 01:33:16.148319: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-18 01:33:16.148344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NamThien): /proc/driver/nvidia/version does not exist\n",
      "2023-12-18 01:33:16.149023: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 01:33:16.882593: I tensorflow_io/core/kernels/kafka_kernels.cc:349] Kafka tail: 180000\n"
     ]
    }
   ],
   "source": [
    "def decode_kafka_item(item):\n",
    "  message = tf.io.decode_csv(item.message, [[0.0] for i in range(NUM_COLUMNS)])\n",
    "  key = tf.strings.to_number(item.key)\n",
    "  return (message, key)\n",
    "\n",
    "BATCH_SIZE=64\n",
    "SHUFFLE_BUFFER_SIZE=64\n",
    "train_ds = tfio.IODataset.from_kafka('turtorial11-susy-train', partition=0, offset=0)\n",
    "train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "train_ds = train_ds.map(decode_kafka_item)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "OPTIMIZER=\"adam\"\n",
    "LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "METRICS=['accuracy']\n",
    "EPOCHS=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               2432      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,481\n",
      "Trainable params: 68,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# design/build the model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input(shape=(NUM_COLUMNS,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.4),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.4),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leducmanh/.local/lib/python3.10/site-packages/tensorflow_io/python/experimental/kafka_group_io_dataset_ops.py:177: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From /home/leducmanh/.local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/leducmanh/.local/lib/python3.10/site-packages/tensorflow_io/python/experimental/kafka_group_io_dataset_ops.py:187: take_while (from tensorflow.python.data.experimental.ops.take_while_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.take_while(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:33:47.579871: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: session.timeout.ms=7000\n",
      "2023-12-18 01:33:47.579974: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: max.poll.interval.ms=8000\n",
      "2023-12-18 01:33:47.579996: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: auto.offset.reset=earliest\n",
      "2023-12-18 01:33:47.580004: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: group.id=testcg\n",
      "2023-12-18 01:33:47.580009: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: bootstrap.servers=127.0.0.1:9092\n",
      "2023-12-18 01:33:47.580028: I tensorflow_io/core/kernels/kafka_kernels.cc:919] max num of messages per batch: 10000\n",
      "2023-12-18 01:33:47.580037: I tensorflow_io/core/kernels/kafka_kernels.cc:938] Creating the kafka consumer\n",
      "2023-12-18 01:33:47.614140: I tensorflow_io/core/kernels/kafka_kernels.cc:945] Subscribing to the kafka topic: turtorial11-susy-test\n"
     ]
    }
   ],
   "source": [
    "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
    "    topics=[\"turtorial11-susy-test\"],\n",
    "    group_id=\"testcg\",\n",
    "    servers=\"127.0.0.1:9092\",\n",
    "    stream_timeout=10000,\n",
    "    configuration=[\n",
    "        \"session.timeout.ms=7000\",\n",
    "        \"max.poll.interval.ms=8000\",\n",
    "        \"auto.offset.reset=earliest\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "def decode_kafka_test_item(raw_message, raw_key):\n",
    "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
    "  key = tf.strings.to_number(raw_key)\n",
    "  return (message, key)\n",
    "\n",
    "test_ds = test_ds.map(decode_kafka_test_item)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leducmanh/.local/lib/python3.10/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-12-18 01:33:52.473042: E tensorflow_io/core/kernels/kafka_kernels.cc:774] REBALANCE: Local: Assign partitions\n",
      "2023-12-18 01:33:52.487363: E tensorflow_io/core/kernels/kafka_kernels.cc:776] Retrieved committed offsets with status code: 0\n",
      "2023-12-18 01:33:52.487428: I tensorflow_io/core/kernels/kafka_kernels.cc:787] REBALANCE: turtorial11-susy-test[0], OFFSET: -1001 ERROR_CODE: 0\n",
      "2023-12-18 01:33:52.487437: I tensorflow_io/core/kernels/kafka_kernels.cc:787] REBALANCE: turtorial11-susy-test[1], OFFSET: -1001 ERROR_CODE: 0\n",
      "2023-12-18 01:33:52.487441: I tensorflow_io/core/kernels/kafka_kernels.cc:802] REBALANCE: Assigning partitions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    666/Unknown - 3s 3ms/step - loss: 0.7211 - accuracy: 0.5473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:33:54.628805: I tensorflow_io/core/kernels/kafka_kernels.cc:996] EOF reached for all 2 partition(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.7235 - accuracy: 0.5394\n",
      "test loss, test acc: [0.7235473394393921, 0.5393916964530945]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:34:05.230525: E tensorflow_io/core/kernels/kafka_kernels.cc:1001] Local: Timed out\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_ds)\n",
    "print(\"test loss, test acc:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --group testcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:34:59.728420: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: session.timeout.ms=7000\n",
      "2023-12-18 01:34:59.728481: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: max.poll.interval.ms=8000\n",
      "2023-12-18 01:34:59.728499: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: auto.offset.reset=earliest\n",
      "2023-12-18 01:34:59.728507: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: group.id=cgonline\n",
      "2023-12-18 01:34:59.728513: I tensorflow_io/core/kernels/kafka_kernels.cc:879] Kafka configuration: bootstrap.servers=127.0.0.1:9092\n",
      "2023-12-18 01:34:59.728532: I tensorflow_io/core/kernels/kafka_kernels.cc:919] max num of messages per batch: 10000\n",
      "2023-12-18 01:34:59.728539: I tensorflow_io/core/kernels/kafka_kernels.cc:938] Creating the kafka consumer\n",
      "2023-12-18 01:34:59.729084: I tensorflow_io/core/kernels/kafka_kernels.cc:945] Subscribing to the kafka topic: turtorial11-susy-train\n"
     ]
    }
   ],
   "source": [
    "online_train_ds = tfio.experimental.streaming.KafkaBatchIODataset(\n",
    "    topics=[\"turtorial11-susy-train\"],\n",
    "    group_id=\"cgonline\",\n",
    "    servers=\"127.0.0.1:9092\",\n",
    "    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.\n",
    "    configuration=[\n",
    "        \"session.timeout.ms=7000\",\n",
    "        \"max.poll.interval.ms=8000\",\n",
    "        \"auto.offset.reset=earliest\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:35:04.581081: E tensorflow_io/core/kernels/kafka_kernels.cc:774] REBALANCE: Local: Assign partitions\n",
      "2023-12-18 01:35:04.582564: E tensorflow_io/core/kernels/kafka_kernels.cc:776] Retrieved committed offsets with status code: 0\n",
      "2023-12-18 01:35:04.582623: I tensorflow_io/core/kernels/kafka_kernels.cc:787] REBALANCE: turtorial11-susy-train[0], OFFSET: -1001 ERROR_CODE: 0\n",
      "2023-12-18 01:35:04.582631: I tensorflow_io/core/kernels/kafka_kernels.cc:802] REBALANCE: Assigning partitions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 0.5439 - accuracy: 0.7210\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4821 - accuracy: 0.7759\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4716 - accuracy: 0.7818\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4644 - accuracy: 0.7832\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4578 - accuracy: 0.7895\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4523 - accuracy: 0.7908\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4594 - accuracy: 0.7885\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4550 - accuracy: 0.7873\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4527 - accuracy: 0.7916\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4605 - accuracy: 0.7884\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4591 - accuracy: 0.7861\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4553 - accuracy: 0.7869\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4575 - accuracy: 0.7901\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4528 - accuracy: 0.7918\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4500 - accuracy: 0.7928\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4486 - accuracy: 0.7921\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4447 - accuracy: 0.7937\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4425 - accuracy: 0.7962\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4492 - accuracy: 0.7935\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4451 - accuracy: 0.7955\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4441 - accuracy: 0.7955\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4493 - accuracy: 0.7932\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4437 - accuracy: 0.7926\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4441 - accuracy: 0.7951\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4602 - accuracy: 0.7858\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4553 - accuracy: 0.7914\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4521 - accuracy: 0.7895\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4460 - accuracy: 0.7947\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4388 - accuracy: 0.7996\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4406 - accuracy: 0.8009\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4571 - accuracy: 0.7893\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4514 - accuracy: 0.7907\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4478 - accuracy: 0.7903\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4488 - accuracy: 0.7865\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4466 - accuracy: 0.7877\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4445 - accuracy: 0.7890\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4428 - accuracy: 0.7988\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4379 - accuracy: 0.8011\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4348 - accuracy: 0.8027\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4497 - accuracy: 0.7949\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4464 - accuracy: 0.7953\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4417 - accuracy: 0.7988\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4390 - accuracy: 0.8006\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4315 - accuracy: 0.8019\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4312 - accuracy: 0.7991\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4492 - accuracy: 0.7872\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4497 - accuracy: 0.7910\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4423 - accuracy: 0.7913\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4534 - accuracy: 0.7870\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4516 - accuracy: 0.7892\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4480 - accuracy: 0.7922\n",
      "Epoch 1/3\n",
      " 30/313 [=>............................] - ETA: 1s - loss: 0.4748 - accuracy: 0.7771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:36:08.210725: I tensorflow_io/core/kernels/kafka_kernels.cc:996] EOF reached for all 1 partition(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4478 - accuracy: 0.7960\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4440 - accuracy: 0.8001\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4373 - accuracy: 0.7993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:36:21.890249: E tensorflow_io/core/kernels/kafka_kernels.cc:1001] Local: Timed out\n"
     ]
    }
   ],
   "source": [
    "def decode_kafka_online_item(raw_message, raw_key):\n",
    "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
    "  key = tf.strings.to_number(raw_key)\n",
    "  return (message, key)\n",
    "  \n",
    "for mini_ds in online_train_ds:\n",
    "  mini_ds = mini_ds.shuffle(buffer_size=32)\n",
    "  mini_ds = mini_ds.map(decode_kafka_online_item)\n",
    "  mini_ds = mini_ds.batch(32)\n",
    "  if len(mini_ds) > 0:\n",
    "    model.fit(mini_ds, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
